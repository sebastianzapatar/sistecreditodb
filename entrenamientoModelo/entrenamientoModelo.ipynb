{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093bec39-6bc5-4b79-a334-1cca1a9ed589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuraci√≥n s√∫per simple con Access Key\n",
    "storage_account_name = \"sistecreditofinal\"\n",
    "storage_account_access_key = \"YpYHNOKME38oGXISqD7KFinQ3arvr43JNX59hiWXyTQvj8O7MwMlRQAx/jrPE2bMY+NHAIC0Sub7+AStbzR/Bg==\"\n",
    "container_name = \"raw\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f87ac3-bb1c-480a-b0ae-10af141d1797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Leer datos (usando el m√©todo h√≠brido que ya funciona)\n",
    "print(\"üìñ Cargando datos...\")\n",
    "file_path = \"abfss://raw@sistecreditofinal.dfs.core.windows.net/data/v1/credit_risk_dataset.csv\"\n",
    "spark_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "df = spark_df.toPandas()\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "print(\"Columnas:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531a6dc2-29b3-4742-a740-9054d5f03655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def explore_credit_data(df):\n",
    "    \"\"\"An√°lisis exploratorio espec√≠fico para datos de cr√©dito\"\"\"\n",
    "    \n",
    "    print(\"üîç === AN√ÅLISIS EXPLORATORIO ===\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(f\"Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # Informaci√≥n general\n",
    "    print(\"\\nInformaci√≥n del dataset:\")\n",
    "    df.info()\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Valores nulos\n",
    "    print(\"\\nValores nulos por columna:\")\n",
    "    nulls = df.isnull().sum()\n",
    "    if nulls.sum() > 0:\n",
    "        print(nulls[nulls > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ No hay valores nulos\")\n",
    "    \n",
    "    # Distribuci√≥n de la variable objetivo (asumiendo que existe)\n",
    "    target_cols = [col for col in df.columns if 'default' in col.lower() or 'risk' in col.lower() or 'target' in col.lower()]\n",
    "    if target_cols:\n",
    "        target_col = target_cols[0]\n",
    "        print(f\"\\nDistribuci√≥n de la variable objetivo '{target_col}':\")\n",
    "        print(df[target_col].value_counts())\n",
    "        print(\"Porcentaje:\")\n",
    "        print(df[target_col].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    return target_col if target_cols else None\n",
    "\n",
    "# Ejecutar EDA\n",
    "target_column = explore_credit_data(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "entrenamientoModelo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
