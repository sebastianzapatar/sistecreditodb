{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093bec39-6bc5-4b79-a334-1cca1a9ed589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuraci√≥n s√∫per simple con Access Key\n",
    "storage_account_name = \"sistecreditofinal\"\n",
    "storage_account_access_key = \"YpYHNOKME38oGXISqD7KFinQ3arvr43JNX59hiWXyTQvj8O7MwMlRQAx/jrPE2bMY+NHAIC0Sub7+AStbzR/Bg==\"\n",
    "container_name = \"raw\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1a7fde-8d6b-4523-b5a1-5e275f2a6032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === CREAR ARCHIVO .env DIRECTAMENTE EN DATABRICKS ===\n",
    "\n",
    "# Contenido de tu archivo .env\n",
    "env_content = \"\"\"AZURE_STORAGE_ACCOUNT=sistecreditofinal\n",
    "AZURE_STORAGE_KEY=YpYHNOKME38oGXISqD7KFinQ3arvr43JNX59hiWXyTQvj8O7MwMlRQAx/jrPE2bMY+NHAIC0Sub7+AStbzR/Bg==\"\"\"\n",
    "\n",
    "# Crear directorio si no existe\n",
    "dbutils.fs.mkdirs(\"/Users/pansezapata@gmail.com/\")\n",
    "\n",
    "# Escribir archivo .env a DBFS\n",
    "result = dbutils.fs.put(\"/Users/pansezapata@gmail.com/.env\", env_content, overwrite=True)\n",
    "\n",
    "print(f\"Archivo .env creado: {result}\")\n",
    "\n",
    "# Verificar que existe\n",
    "files = dbutils.fs.ls(\"/Users/pansezapata@gmail.com/\")\n",
    "print(\"Archivos en tu directorio:\")\n",
    "for file in files:\n",
    "    print(f\"  - {file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e08ae9-6781-406d-9b2e-e45389ab8d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === LEER ARCHIVO .env DE FORMA SEGURA ===\n",
    "\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Ruta correcta (sin /dbfs al inicio para load_dotenv)\n",
    "env_path = \"/dbfs/Users/pansezapata@gmail.com/.env\"\n",
    "\n",
    "# Cargar variables desde .env\n",
    "result = load_dotenv(env_path)\n",
    "print(f\".env cargado: {result}\")\n",
    "\n",
    "# Leer variables de entorno\n",
    "storage_account = os.getenv('AZURE_STORAGE_ACCOUNT')\n",
    "storage_key = os.getenv('AZURE_STORAGE_KEY')\n",
    "\n",
    "print(f\"Storage Account: {storage_account}\")\n",
    "print(f\"Storage Key: {'*' * 12} (protegida)\")\n",
    "\n",
    "# Verificar que no sean None\n",
    "if storage_account and storage_key:\n",
    "    # Configurar Spark\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "        storage_key\n",
    "    )\n",
    "    print(\"Configuraci√≥n Spark completada!\")\n",
    "else:\n",
    "    print(\"Error: Variables de entorno est√°n vac√≠as\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f87ac3-bb1c-480a-b0ae-10af141d1797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Leer datos (usando el m√©todo h√≠brido que ya funciona)\n",
    "print(\"üìñ Cargando datos...\")\n",
    "file_path = \"abfss://raw@sistecreditofinal.dfs.core.windows.net/data/v1/credit_risk_dataset.csv\"\n",
    "spark_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "df = spark_df.toPandas()\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "print(\"Columnas:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531a6dc2-29b3-4742-a740-9054d5f03655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def explore_credit_data(df):\n",
    "    \"\"\"An√°lisis exploratorio espec√≠fico para datos de cr√©dito\"\"\"\n",
    "    \n",
    "    print(\"üîç === AN√ÅLISIS EXPLORATORIO ===\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(f\"Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # Informaci√≥n general\n",
    "    print(\"\\nInformaci√≥n del dataset:\")\n",
    "    df.info()\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Valores nulos\n",
    "    print(\"\\nValores nulos por columna:\")\n",
    "    nulls = df.isnull().sum()\n",
    "    if nulls.sum() > 0:\n",
    "        print(nulls[nulls > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ No hay valores nulos\")\n",
    "    \n",
    "    # Distribuci√≥n de la variable objetivo (asumiendo que existe)\n",
    "    target_cols = [col for col in df.columns if 'default' in col.lower() or 'risk' in col.lower() or 'target' in col.lower()]\n",
    "    if target_cols:\n",
    "        target_col = target_cols[0]\n",
    "        print(f\"\\nDistribuci√≥n de la variable objetivo '{target_col}':\")\n",
    "        print(df[target_col].value_counts())\n",
    "        print(\"Porcentaje:\")\n",
    "        print(df[target_col].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    return target_col if target_cols else None\n",
    "\n",
    "# Ejecutar EDA\n",
    "target_column = explore_credit_data(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "entrenamientoModelo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
