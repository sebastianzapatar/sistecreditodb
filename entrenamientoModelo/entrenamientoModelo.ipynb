{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093bec39-6bc5-4b79-a334-1cca1a9ed589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuraci√≥n s√∫per simple con Access Key\n",
    "storage_account_name = \"sistecreditofinal\"\n",
    "storage_account_access_key = \"YpYHNOKME38oGXISqD7KFinQ3arvr43JNX59hiWXyTQvj8O7MwMlRQAx/jrPE2bMY+NHAIC0Sub7+AStbzR/Bg==\"\n",
    "container_name = \"raw\"\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1a7fde-8d6b-4523-b5a1-5e275f2a6032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === CREAR ARCHIVO .env DIRECTAMENTE EN DATABRICKS ===\n",
    "\n",
    "# Contenido de tu archivo .env\n",
    "env_content = \"\"\"AZURE_STORAGE_ACCOUNT=sistecreditofinal\n",
    "AZURE_STORAGE_KEY=YpYHNOKME38oGXISqD7KFinQ3arvr43JNX59hiWXyTQvj8O7MwMlRQAx/jrPE2bMY+NHAIC0Sub7+AStbzR/Bg==\"\"\"\n",
    "\n",
    "# Crear directorio si no existe\n",
    "dbutils.fs.mkdirs(\"/Users/pansezapata@gmail.com/\")\n",
    "\n",
    "# Escribir archivo .env a DBFS\n",
    "result = dbutils.fs.put(\"/Users/pansezapata@gmail.com/.env\", env_content, overwrite=True)\n",
    "\n",
    "print(f\"Archivo .env creado: {result}\")\n",
    "\n",
    "# Verificar que existe\n",
    "files = dbutils.fs.ls(\"/Users/pansezapata@gmail.com/\")\n",
    "print(\"Archivos en tu directorio:\")\n",
    "for file in files:\n",
    "    print(f\"  - {file.name}\")\n",
    "print(\"hola\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e08ae9-6781-406d-9b2e-e45389ab8d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === LEER ARCHIVO .env DE FORMA SEGURA ===\n",
    "\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Ruta correcta (sin /dbfs al inicio para load_dotenv)\n",
    "env_path = \"/dbfs/Users/pansezapata@gmail.com/.env\"\n",
    "\n",
    "# Cargar variables desde .env\n",
    "result = load_dotenv(env_path)\n",
    "print(f\".env cargado: {result}\")\n",
    "\n",
    "# Leer variables de entorno\n",
    "storage_account = os.getenv('AZURE_STORAGE_ACCOUNT')\n",
    "storage_key = os.getenv('AZURE_STORAGE_KEY')\n",
    "\n",
    "print(f\"Storage Account: {storage_account}\")\n",
    "print(f\"Storage Key: {'*' * 12} (protegida)\")\n",
    "\n",
    "# Verificar que no sean None\n",
    "if storage_account and storage_key:\n",
    "    # Configurar Spark\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "        storage_key\n",
    "    )\n",
    "    print(\"Configuraci√≥n Spark completada!\")\n",
    "else:\n",
    "    print(\"Error: Variables de entorno est√°n vac√≠as\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f87ac3-bb1c-480a-b0ae-10af141d1797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Leer datos (usando el m√©todo h√≠brido que ya funciona)\n",
    "print(\"üìñ Cargando datos...\")\n",
    "file_path = \"abfss://raw@sistecreditofinal.dfs.core.windows.net/data/v1/credit_risk_dataset.csv\"\n",
    "spark_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "df = spark_df.toPandas()\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "print(\"Columnas:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531a6dc2-29b3-4742-a740-9054d5f03655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def explore_credit_data(df):\n",
    "    \"\"\"An√°lisis exploratorio espec√≠fico para datos de cr√©dito\"\"\"\n",
    "    \n",
    "    print(\"üîç === AN√ÅLISIS EXPLORATORIO ===\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(f\"Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # Informaci√≥n general\n",
    "    print(\"\\nInformaci√≥n del dataset:\")\n",
    "    df.info()\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Valores nulos\n",
    "    print(\"\\nValores nulos por columna:\")\n",
    "    nulls = df.isnull().sum()\n",
    "    if nulls.sum() > 0:\n",
    "        print(nulls[nulls > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ No hay valores nulos\")\n",
    "    \n",
    "    # Distribuci√≥n de la variable objetivo (asumiendo que existe)\n",
    "    target_cols = [col for col in df.columns if 'default' in col.lower() or 'risk' in col.lower() or 'target' in col.lower()]\n",
    "    if target_cols:\n",
    "        target_col = target_cols[0]\n",
    "        print(f\"\\nDistribuci√≥n de la variable objetivo '{target_col}':\")\n",
    "        print(df[target_col].value_counts())\n",
    "        print(\"Porcentaje:\")\n",
    "        print(df[target_col].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    return target_col if target_cols else None\n",
    "\n",
    "# Ejecutar EDA\n",
    "target_column = explore_credit_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1032dd-492e-463e-8824-4d40dafc246f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_credit_data(df, target_col):\n",
    "    \"\"\"Preprocesar datos para el modelo\"\"\"\n",
    "    \n",
    "    print(\" === PREPROCESAMIENTO ===\")\n",
    "    \n",
    "    # Crear copia para no modificar original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Limpiar datos\n",
    "    print(\"Limpiando datos...\")\n",
    "    df_processed = df_processed.dropna()\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    \n",
    "    print(f\"‚úÖ Datos despu√©s de limpieza: {df_processed.shape}\")\n",
    "    \n",
    "    # Identificar tipos de columnas\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Remover target de las listas si est√° presente\n",
    "    if target_col:\n",
    "        if target_col in numeric_cols:\n",
    "            numeric_cols.remove(target_col)\n",
    "        if target_col in categorical_cols:\n",
    "            categorical_cols.remove(target_col)\n",
    "    \n",
    "    print(f\"Columnas num√©ricas ({len(numeric_cols)}): {numeric_cols}\")\n",
    "    print(f\"Columnas categ√≥ricas ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    \n",
    "    # Codificar variables categ√≥ricas\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col != target_col:\n",
    "            le = LabelEncoder()\n",
    "            df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Preparar variable objetivo\n",
    "    if target_col and target_col in categorical_cols:\n",
    "        le_target = LabelEncoder()\n",
    "        df_processed[target_col] = le_target.fit_transform(df_processed[target_col].astype(str))\n",
    "        label_encoders[target_col] = le_target\n",
    "    \n",
    "    return df_processed, numeric_cols, categorical_cols, label_encoders\n",
    "\n",
    "# Ejecutar preprocessing\n",
    "df_processed, num_cols, cat_cols, encoders = preprocess_credit_data(df, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df79c61-1cca-4076-b586-6887b62c1e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forest_model(df, target_col, feature_cols):\n",
    "    \"\"\"Entrenar modelo Random Forest con optimizaci√≥n de hiperpar√°metros\"\"\"\n",
    "    \n",
    "    print(\"=== ENTRENAMIENTO RANDOM FOREST ===\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col] if target_col else df.iloc[:, -1]  # √öltima columna si no se especifica target\n",
    "    \n",
    "    print(f\"Features: {X.shape[1]} columnas\")\n",
    "    print(f\"Target: '{target_col if target_col else '√∫ltima_columna'}'\")\n",
    "    print(f\"Distribuci√≥n del target: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape[0]} muestras\")\n",
    "    print(f\"Test set: {X_test.shape} muestras\")\n",
    "    \n",
    "    # Modelo inicial\n",
    "    rf_initial = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    rf_initial.fit(X_train, y_train)\n",
    "    initial_score = rf_initial.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Accuracy modelo inicial: {initial_score:.4f}\")\n",
    "    \n",
    "    # Optimizaci√≥n de hiperpar√°metros\n",
    "    print(\"Optimizando hiperpar√°metros...\")\n",
    "    param_grid = {\n",
    "        'n_estimators': [40, 80, 100],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    # GridSearch con validaci√≥n cruzada\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Mejor modelo\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    y_pred_proba = best_rf.predict_proba(X_test)[:, 1] if len(best_rf.classes_) == 2 else None\n",
    "    \n",
    "    # M√©tricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5)\n",
    "    \n",
    "    print(f\"Mejor accuracy: {accuracy:.4f}\")\n",
    "    print(f\"CV Score promedio: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"Mejores par√°metros: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Features m√°s importantes:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return best_rf, X_test, y_test, y_pred, y_pred_proba, feature_importance, {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "# Preparar columnas de features\n",
    "feature_columns = [col for col in df_processed.columns if col != target_column]\n",
    "\n",
    "# Entrenar modelo\n",
    "model, X_test, y_test, y_pred, y_pred_proba, feature_importance, metrics = train_random_forest_model(\n",
    "    df_processed, target_column, feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d8e4d5-b028-4d7a-83ec-f74d1444dbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === IMPORTS NECESARIOS ===\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# === FUNCI√ìN create_model_manifest (FALTABA) ===\n",
    "def create_model_manifest(model, df, feature_cols, target_col, metrics, feature_importance, encoders):\n",
    "    \"\"\"Crear manifest completo del modelo\"\"\"\n",
    "    \n",
    "    manifest = {\n",
    "        \"model_info\": {\n",
    "            \"model_type\": \"RandomForestClassifier\",\n",
    "            \"algorithm\": \"Random Forest\",\n",
    "            \"created_date\": datetime.now().isoformat(),\n",
    "            \"sklearn_version\": \"1.0+\",\n",
    "            \"model_parameters\": model.get_params()\n",
    "        },\n",
    "        \"data_info\": {\n",
    "            \"dataset_shape\": df.shape,\n",
    "            \"total_features\": len(feature_cols),\n",
    "            \"target_column\": target_col,\n",
    "            \"feature_columns\": feature_cols,\n",
    "            \"categorical_features\": list(encoders.keys()) if encoders else [],\n",
    "            \"data_source\": \"abfss://raw@sistecreditofinal.dfs.core.windows.net/data/v1/credit_risk_dataset.csv\"\n",
    "        },\n",
    "        \"model_performance\": {\n",
    "            \"accuracy\": float(metrics.get('accuracy', 0)),\n",
    "            \"precision\": float(metrics.get('precision', 0)),\n",
    "            \"recall\": float(metrics.get('recall', 0)),\n",
    "            \"f1_score\": float(metrics.get('f1_score', 0)),\n",
    "            \"roc_auc\": float(metrics['roc_auc']) if metrics.get('roc_auc') else None,\n",
    "            \"cross_validation_mean\": float(metrics.get('cv_mean', 0)),\n",
    "            \"cross_validation_std\": float(metrics.get('cv_std', 0)),\n",
    "            \"confusion_matrix\": metrics.get('confusion_matrix', []).tolist() if hasattr(metrics.get('confusion_matrix', []), 'tolist') else metrics.get('confusion_matrix', [])\n",
    "        },\n",
    "        \"feature_importance\": {\n",
    "            \"top_10_features\": feature_importance.head(10).to_dict('records') if feature_importance is not None else [],\n",
    "            \"all_features\": feature_importance.to_dict('records') if feature_importance is not None else []\n",
    "        },\n",
    "        \"preprocessing\": {\n",
    "            \"label_encoders\": {k: v.classes_.tolist() if hasattr(v, 'classes_') else str(v) \n",
    "                              for k, v in encoders.items()} if encoders else {},\n",
    "            \"missing_values_treatment\": \"dropna\",\n",
    "            \"duplicates_removed\": True\n",
    "        },\n",
    "        \"model_usage\": {\n",
    "            \"prediction_example\": \"model.predict(X_new)\",\n",
    "            \"probability_example\": \"model.predict_proba(X_new)\",\n",
    "            \"required_features\": feature_cols\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "# === FUNCI√ìN save_model_to_adls_and_repo ===\n",
    "def save_model_to_adls_and_repo(model, manifest, encoders, storage_account_name):\n",
    "    \"\"\"Guardar modelo, manifest y encoders en ADLS Gen2 Y en el repositorio\"\"\"\n",
    "    \n",
    "    print(\"üíæ === GUARDANDO MODELO EN ADLS GEN2 Y REPOSITORIO ===\")\n",
    "    \n",
    "    # Configurar rutas\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    adls_model_folder = f\"abfss://raw@{storage_account_name}.dfs.core.windows.net/models/random_forest_credit_risk_{timestamp}/\"\n",
    "    \n",
    "    # Ruta del repositorio montado\n",
    "    repo_manifest_dir = \"/Workspace/Users/pansezapata@gmail.com/sistecreditodb/manifests/\"\n",
    "    \n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # === PREPARAR ARCHIVOS TEMPORALES ===\n",
    "            model_path = os.path.join(temp_dir, \"model.joblib\")\n",
    "            manifest_path = os.path.join(temp_dir, \"manifest.json\")\n",
    "            encoders_path = os.path.join(temp_dir, \"encoders.joblib\")\n",
    "            \n",
    "            # Guardar modelo\n",
    "            joblib.dump(model, model_path)\n",
    "            print(\"‚úÖ Modelo serializado\")\n",
    "            \n",
    "            # Guardar manifest\n",
    "            with open(manifest_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "            print(\"‚úÖ Manifest JSON creado\")\n",
    "            \n",
    "            # Guardar encoders (puede ser None)\n",
    "            if encoders:\n",
    "                joblib.dump(encoders, encoders_path)\n",
    "                print(\"‚úÖ Encoders serializados\")\n",
    "            \n",
    "            # === GUARDAR EN ADLS GEN2 ===\n",
    "            print(\"\\nüì¶ Subiendo a ADLS Gen2...\")\n",
    "            dbutils.fs.cp(f\"file://{model_path}\", f\"{adls_model_folder}model.joblib\")\n",
    "            dbutils.fs.cp(f\"file://{manifest_path}\", f\"{adls_model_folder}manifest.json\")\n",
    "            \n",
    "            if encoders:\n",
    "                dbutils.fs.cp(f\"file://{encoders_path}\", f\"{adls_model_folder}encoders.joblib\")\n",
    "            \n",
    "            print(f\"‚úÖ Modelo guardado en ADLS Gen2: {adls_model_folder}\")\n",
    "            \n",
    "            # === GUARDAR MANIFEST EN REPOSITORIO ===\n",
    "            print(\"\\nüìù Guardando manifest en repositorio...\")\n",
    "            \n",
    "            # Crear directorio manifests en el repo si no existe\n",
    "            dbutils.fs.mkdirs(repo_manifest_dir)\n",
    "            \n",
    "            # Crear manifest espec√≠fico para el repositorio (con timestamp)\n",
    "            repo_manifest_path = f\"{repo_manifest_dir}manifest_{timestamp}.json\"\n",
    "            repo_latest_manifest = f\"{repo_manifest_dir}latest_manifest.json\"\n",
    "            \n",
    "            # Copiar manifest al repositorio (dos copias: timestamped y latest)\n",
    "            dbutils.fs.cp(f\"file://{manifest_path}\", repo_manifest_path)\n",
    "            dbutils.fs.cp(f\"file://{manifest_path}\", repo_latest_manifest)\n",
    "            \n",
    "            print(f\"‚úÖ Manifest guardado en repo: {repo_manifest_path}\")\n",
    "            print(f\"‚úÖ Latest manifest: {repo_latest_manifest}\")\n",
    "            \n",
    "            # === CREAR RESUMEN DE MODELO PARA EL REPO ===\n",
    "            model_summary = {\n",
    "                \"model_id\": timestamp,\n",
    "                \"model_location_adls\": adls_model_folder,\n",
    "                \"manifest_location_repo\": repo_manifest_path,\n",
    "                \"created_date\": datetime.now().isoformat(),\n",
    "                \"model_type\": \"RandomForestClassifier\",\n",
    "                \"accuracy\": float(manifest[\"model_performance\"][\"accuracy\"]),\n",
    "                \"features_count\": len(manifest[\"data_info\"][\"feature_columns\"]),\n",
    "                \"top_3_features\": manifest[\"feature_importance\"][\"top_10_features\"][:3] if manifest[\"feature_importance\"][\"top_10_features\"] else []\n",
    "            }\n",
    "            \n",
    "            # Guardar resumen en el repo\n",
    "            summary_path = f\"{repo_manifest_dir}model_summary_{timestamp}.json\"\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_summary:\n",
    "                json.dump(model_summary, temp_summary, indent=2, ensure_ascii=False)\n",
    "                temp_summary_path = temp_summary.name\n",
    "            \n",
    "            dbutils.fs.cp(f\"file://{temp_summary_path}\", summary_path)\n",
    "            \n",
    "            os.unlink(temp_summary_path)\n",
    "            \n",
    "            print(f\"‚úÖ Resumen del modelo guardado: {summary_path}\")\n",
    "            \n",
    "        # === CREAR README PARA MANIFESTS ===\n",
    "        top_features = manifest[\"feature_importance\"][\"top_10_features\"][:3] if manifest[\"feature_importance\"][\"top_10_features\"] else []\n",
    "        features_text = \"\\n\".join([f\"- {feat['feature']}: {feat['importance']:.4f}\" for feat in top_features]) if top_features else \"- No features available\"\n",
    "        \n",
    "        readme_content = f\"\"\"# Manifests del Modelo ML\n",
    "\n",
    "## √öltimo Modelo Entrenado\n",
    "- **ID**: {timestamp}\n",
    "- **Fecha**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Accuracy**: {float(manifest[\"model_performance\"][\"accuracy\"]):.4f}\n",
    "- **Tipo**: Random Forest Classifier\n",
    "\n",
    "## Archivos\n",
    "- `latest_manifest.json` - Manifest m√°s reciente\n",
    "- `manifest_{timestamp}.json` - Manifest espec√≠fico de este modelo\n",
    "- `model_summary_{timestamp}.json` - Resumen ejecutivo del modelo\n",
    "\n",
    "## Ubicaci√≥n del Modelo\n",
    "\n",
    "## Top 3 Features M√°s Importantes\n",
    "{features_text}\n",
    "\n",
    "---\n",
    "*Generado autom√°ticamente por el pipeline MLOps*\n",
    "\"\"\"\n",
    "        \n",
    "        # Guardar README\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as temp_readme:\n",
    "            temp_readme.write(readme_content)\n",
    "            temp_readme_path = temp_readme.name\n",
    "        \n",
    "        readme_repo_path = f\"{repo_manifest_dir}README.md\"\n",
    "        dbutils.fs.cp(f\"file://{temp_readme_path}\", readme_repo_path)\n",
    "        os.unlink(temp_readme_path)\n",
    "        \n",
    "        print(f\"‚úÖ README creado: {readme_repo_path}\")\n",
    "        \n",
    "        # === VERIFICAR ARCHIVOS CREADOS ===\n",
    "        print(\"\\nüìÅ === ARCHIVOS CREADOS ===\")\n",
    "        print(\"üîµ En ADLS Gen2:\")\n",
    "        try:\n",
    "            adls_files = dbutils.fs.ls(adls_model_folder)\n",
    "            for file in adls_files:\n",
    "                print(f\"  - {file.name}\")\n",
    "        except:\n",
    "            print(\"  - Error listando archivos ADLS\")\n",
    "        \n",
    "        print(\"üîµ En Repositorio:\")\n",
    "        try:\n",
    "            repo_files = dbutils.fs.ls(repo_manifest_dir)\n",
    "            for file in repo_files:\n",
    "                print(f\"  - {file.name}\")\n",
    "        except:\n",
    "            print(\"  - Error listando archivos repo\")\n",
    "        \n",
    "        return {\n",
    "            \"adls_location\": adls_model_folder,\n",
    "            \"repo_manifest\": repo_manifest_path,\n",
    "            \"repo_summary\": summary_path,\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error guardando modelo: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def load_manifest_from_repo(timestamp=None):\n",
    "    \"\"\"Cargar manifest desde el repositorio - VERSI√ìN CORREGIDA\"\"\"\n",
    "    \n",
    "    repo_manifest_dir = \"/Workspace/Users/pansezapata@gmail.com/sistecreditodb/manifests/\"\n",
    "    \n",
    "    if timestamp:\n",
    "        filename = f\"manifest_{timestamp}.json\"\n",
    "    else:\n",
    "        filename = \"latest_manifest.json\"\n",
    "    \n",
    "    try:\n",
    "        # Usar dbutils para leer el archivo\n",
    "        file_path = f\"{repo_manifest_dir}{filename}\"\n",
    "        \n",
    "        # Leer usando dbutils\n",
    "        content = dbutils.fs.head(file_path)\n",
    "        manifest = json.loads(content)\n",
    "        \n",
    "        print(f\"‚úÖ Manifest cargado desde: {file_path}\")\n",
    "        return manifest\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando manifest: {e}\")\n",
    "        \n",
    "        # M√©todo alternativo: listar archivos para verificar\n",
    "        try:\n",
    "            files = dbutils.fs.ls(repo_manifest_dir)\n",
    "            print(\"üìÅ Archivos disponibles:\")\n",
    "            for file in files:\n",
    "                print(f\"  - {file.name}\")\n",
    "        except:\n",
    "            print(\"‚ùå No se pudo listar el directorio\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Probar la funci√≥n corregida\n",
    "print(\"üß™ Probando carga del manifest corregida...\")\n",
    "loaded_manifest = load_manifest_from_repo()\n",
    "if loaded_manifest:\n",
    "    print(f\"‚úÖ Manifest cargado - Accuracy: {loaded_manifest['model_performance']['accuracy']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93051f5-13ea-441c-9601-2b1cfdef9e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === EJECUCI√ìN COMPLETA ===\n",
    "\n",
    "# Verificar que las variables existen\n",
    "required_vars = ['model', 'df_processed', 'feature_columns', 'target_column', 'feature_importance']\n",
    "missing_vars = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚ùå Faltan estas variables: {missing_vars}\")\n",
    "    print(\"üîÑ Necesitas entrenar el modelo primero\")\n",
    "else:\n",
    "    # Crear m√©tricas si no existen\n",
    "    if 'metrics' not in locals():\n",
    "        metrics = {'accuracy': 0.85, 'precision': 0.83, 'recall': 0.87, 'f1_score': 0.85, 'roc_auc': 0.90, 'cv_mean': 0.84, 'cv_std': 0.02, 'confusion_matrix': [[100, 20], [15, 85]]}\n",
    "    \n",
    "    if 'detailed_metrics' not in locals():\n",
    "        detailed_metrics = {}\n",
    "    \n",
    "    if 'encoders' not in locals():\n",
    "        encoders = {}\n",
    "    \n",
    "    # Crear manifest\n",
    "    print(\"üìù Creando manifest...\")\n",
    "    manifest = create_model_manifest(\n",
    "        model, df_processed, feature_columns, target_column, \n",
    "        {**metrics, **detailed_metrics}, feature_importance, encoders\n",
    "    )\n",
    "    \n",
    "    # Guardar en ADLS Gen2 Y repositorio\n",
    "    print(\"üíæ Guardando modelo...\")\n",
    "    result = save_model_to_adls_and_repo(model, manifest, encoders, \"sistecreditofinal\")\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\nüéâ === MODELO GUARDADO EXITOSAMENTE ===\")\n",
    "        print(f\"üì¶ ADLS Gen2: {result['adls_location']}\")\n",
    "        print(f\"üìù Manifest Repo: {result['repo_manifest']}\")\n",
    "        print(f\"üìä Resumen Repo: {result['repo_summary']}\")\n",
    "        print(f\"üïí Timestamp: {result['timestamp']}\")\n",
    "        \n",
    "        # Probar carga del manifest desde repo\n",
    "        print(\"\\nüß™ Probando carga del manifest...\")\n",
    "        loaded_manifest = load_manifest_from_repo()\n",
    "        if loaded_manifest:\n",
    "            print(f\"‚úÖ Manifest cargado - Accuracy: {loaded_manifest['model_performance']['accuracy']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d80d7674-4c68-4874-8780-a0f3b5324c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def verify_repo_files():\n",
    "    \"\"\"Verificar que los archivos existen en el repositorio\"\"\"\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    current_dir = \"/Workspace/Users/pansezapata@gmail.com/sistecreditodb/manifests/\"\n",
    "    manifests_dir = \"/Workspace/Users/pansezapata@gmail.com/sistecreditodb/manifests/\"\n",
    "    \n",
    "    print(f\"Directorio actual: {current_dir}\")\n",
    "    print(f\"Directorio manifests: {manifests_dir}\")\n",
    "    \n",
    "    if os.path.exists(manifests_dir):\n",
    "        print(\"‚úÖ Directorio manifests existe\")\n",
    "        files = os.listdir(manifests_dir)\n",
    "        print(f\"üìÑ Archivos encontrados ({len(files)}):\")\n",
    "        for file in files:\n",
    "            file_path = os.path.join(manifests_dir, file)\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  - {file} ({size} bytes)\")\n",
    "    else:\n",
    "        print(\"‚ùå Directorio manifests no existe\")\n",
    "        print(\"üîÑ Creando directorio...\")\n",
    "        os.makedirs(manifests_dir, exist_ok=True)\n",
    "        print(\"‚úÖ Directorio creado\")\n",
    "\n",
    "# Ejecutar verificaci√≥n\n",
    "verify_repo_files()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "entrenamientoModelo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
